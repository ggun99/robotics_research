import numpy as np
import cv2
import cv2.aruco as aruco
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, CameraInfo
from geometry_msgs.msg import PoseStamped, TransformStamped
from std_msgs.msg import Header
from cv_bridge import CvBridge
import time
import tf2_ros
from scipy.spatial.transform import Rotation as R
# from tf_transformations import quaternion_from_matrix

class D435ArucoDetector(Node):
    def __init__(self):
        super().__init__('d435_aruco_detector')
        
        # ë¡œë´‡ ë§ˆì»¤ë“¤ (0~3ë²ˆ) - ë¡œì»¬ ì¢Œí‘œê³„
        self.robot_markers_local = {
            0: np.array([-0.045, 0.0565, 0.0]),
            1: np.array([0.045, -0.0565, 0.0]),
            2: np.array([0.045, -0.0565, 0.0]),
            3: np.array([-0.045, 0.0565, 0.0])
        }
    
        self.marker_length = 0.075  # ë§ˆì»¤ í¬ê¸° 5cm
        
        # ì›”ë“œ ì¢Œí‘œê³„ ì„¤ì • ìƒíƒœ
        self.world_established = False
        self.camera_to_world_R = None
        self.camera_to_world_t = None
        
        # ì¹´ë©”ë¼ ì •ë³´
        self.camera_matrix = None
        self.dist_coeffs = None
        self.camera_info_received = False
        
        # ë¡œë´‡ ì¤‘ì‹¬ ì¶”ì 
        self.latest_robot_center = None
        self.center_timestamp = None
        
        self.aruco_50 = [0, 1,2,3]
        self.aruco_75 = [10]

        # ArUco ì„¤ì •
        self.aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_50)
        self.aruco_params = cv2.aruco.DetectorParameters()

        # ë³€í™˜ í–‰ë ¬ë“¤
        self.H_world2cam = None    # 10ë²ˆ ë§ˆì»¤ ê¸°ì¤€ ì›”ë“œâ†’ì¹´ë©”ë¼
        self.H_cam2robot = None    # ì¹´ë©”ë¼â†’ë¡œë´‡ ì¤‘ì‹¬
        self.H_world2robot = None  # ì›”ë“œâ†’ë¡œë´‡
        
        self.tf_broadcaster = tf2_ros.TransformBroadcaster(self)

        # êµ¬ë…ì ì„¤ì •
        self.create_subscription(
            Image, '/camera3/camera3/color/image_raw', 
            self.image_callback, 10
        )
        self.create_subscription(
            CameraInfo, '/camera3/camera3/color/camera_info',
            self.camera_info_callback, 10
        )
        
        # ë°œí–‰ì
        self.robot_center_pub = self.create_publisher(
            PoseStamped, '/robot_center', 10
        )
        
        self.get_logger().info("D435 ArUco Detector initialized")

    def camera_info_callback(self, msg):
        """ì¹´ë©”ë¼ ë‚´ë¶€ ë§¤ê°œë³€ìˆ˜ ìˆ˜ì‹ """
        if not self.camera_info_received:
            self.camera_matrix = np.array(msg.k).reshape(3, 3)
            self.dist_coeffs = np.array(msg.d)
            self.camera_info_received = True
            self.get_logger().info("Camera intrinsics received")

    def image_callback(self, msg):
        """ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ArUco ê²€ì¶œ"""
        if not self.camera_info_received:
            return
            
        bridge = CvBridge()
        cv_image = bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')
        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)

        # ArUco ë§ˆì»¤ ê²€ì¶œ
        detector = aruco.ArucoDetector(self.aruco_dict, self.aruco_params)
        corners, ids, _ = detector.detectMarkers(gray)

        if ids is None:
            return

        # ê¸°ë³¸ í¬ì¦ˆ ì¶”ì • (ì¹´ë©”ë¼ ì¢Œí‘œê³„)
        try:
            rvecs, tvecs, _ = cv2.aruco.estimatePoseSingleMarkers(
                corners, self.marker_length, self.camera_matrix, self.dist_coeffs
            )
        except Exception as e:
            self.get_logger().error(f"Pose estimation failed: {e}")
            return

        # ê²€ì¶œëœ ë§ˆì»¤ ì •ë¦¬
        detected_markers = {}
        for i, marker_id in enumerate(ids.flatten()):
            detected_markers[marker_id] = {
                'cam_tvec': tvecs[i].reshape(3),
                'cam_rvec': rvecs[i].reshape(3),
                'corners': corners[i]
            }

        # ğŸ†• ë‹¨ìˆœí•œ ì›”ë“œ ì¢Œí‘œê³„ ì„¤ì •: 10ë²ˆ ë§ˆì»¤ = ì›”ë“œ ì›ì 
        H_world2cam = None
        robot_markers_cam = {}

        for marker_id, data in detected_markers.items():
            # âœ… 10ë²ˆ ë§ˆì»¤ë¥¼ ì›”ë“œ ì¢Œí‘œê³„ë¡œ ì„¤ì • (ë‹¨ìˆœ!)
            if marker_id == 10:
                rvec = data['cam_rvec']
                tvec = data['cam_tvec']
                H_cam2world = np.eye(4)
                R, _ = cv2.Rodrigues(rvec)
                H_cam2world[0:3, 0:3] = R
                H_cam2world[0:3, 3] = tvec
                # ë§ˆì»¤ 10ì˜ ì¢Œí‘œê³„ = ì›”ë“œ ì¢Œí‘œê³„
                # H_cam2marker10 ì˜ ì—­ë³€í™˜ = H_marker10â†’cam = H_worldâ†’cam
                H_world2cam = np.linalg.inv(H_cam2world)
                self.H_world2cam = H_world2cam  # í´ë˜ìŠ¤ ë³€ìˆ˜ì— ì €ì¥
                self.world_established = True
                self.broadcast_world_transform(H_world2cam, msg.header.stamp)
                self.get_logger().info("âœ… Marker 10 set as world origin")
                self.get_logger().info(f"Marker 10 position (cam): {tvec}")
            
            # ë¡œë´‡ ë§ˆì»¤ë“¤ ìˆ˜ì§‘
            elif marker_id in self.robot_markers_local:
                rvec = data['cam_rvec']
                tvec = data['cam_tvec']
                R, _ = cv2.Rodrigues(rvec)
                H_cam2marker = np.eye(4)
                H_cam2marker[0:3, 0:3] = R
                H_cam2marker[0:3, 3] = tvec
                robot_markers_cam[marker_id] = {
                    'position': tvec,
                    'rotation': R,
                    'transform': H_cam2marker
                }

        # 10ë²ˆ ë§ˆì»¤ê°€ ì—†ìœ¼ë©´ ì›”ë“œ ì¢Œí‘œê³„ ì—†ìŒ
        if not self.world_established:
            self.get_logger().warn("âŒ Marker 10 not detected - no world coordinate system")
            self.visualize_results(cv_image, detected_markers)
            return

        # ğŸ¤– ë¡œë´‡ ì¤‘ì‹¬ ê³„ì‚° (ê°œì„ ë¨)
        if len(robot_markers_cam) > 0:
            # âœ… ê°•ê±´í•œ ë¡œë´‡ ì¤‘ì‹¬ ê³„ì‚°
            result = self.calculate_robot_center_robust(robot_markers_cam)
            if result[0] is not None:
                robot_center_cam, robot_rotation_cam = result
                
                # H_cam2robot
                H_cam2robot = np.eye(4)
                H_cam2robot[0:3, 0:3] = robot_rotation_cam
                H_cam2robot[0:3, 3] = robot_center_cam
                self.H_cam2robot = H_cam2robot
                
                # H_world2robot ê³„ì‚°
                H_world2robot = H_world2cam @ H_cam2robot
                robot_center_world = H_world2robot[:3, 3]
                
                self.broadcast_robot_transform(H_world2robot, msg.header.stamp)
                self.latest_robot_center = robot_center_world
                self.center_timestamp = time.time()
                
                # ê²€ì¶œëœ ë§ˆì»¤ ìˆ˜ ë¡œê·¸
                self.get_logger().info(f"ğŸ¤– Robot center from {len(robot_markers_cam)} markers: {robot_center_world}")
                
                # ë°œí–‰
                self.publish_robot_center(robot_center_world, len(robot_markers_cam))
            else:
                self.get_logger().warn("âŒ Cannot estimate robot center")

        # ì‹œê°í™”
        self.visualize_results(cv_image, detected_markers)

    def project_world_to_image_simple(self, world_point):
        """10ë²ˆ ë§ˆì»¤ ê¸°ì¤€ ì›”ë“œ ì¢Œí‘œë¥¼ ì´ë¯¸ì§€ë¡œ íˆ¬ì˜"""
        
        if not hasattr(self, 'H_world2cam') or self.H_world2cam is None:
            return None
        
        try:
            # ì›”ë“œ â†’ ì¹´ë©”ë¼ ë³€í™˜
            world_point_homogeneous = np.append(world_point, 1)
            cam_point_homogeneous = self.H_world2cam @ world_point_homogeneous
            cam_point = cam_point_homogeneous[:3]
            
            if cam_point[2] <= 0:  # ì¹´ë©”ë¼ ë’¤ì— ìˆìŒ
                return None
            
            # ì¹´ë©”ë¼ â†’ ì´ë¯¸ì§€ íˆ¬ì˜
            image_points, _ = cv2.projectPoints(
                cam_point.reshape(1, 1, 3), np.zeros(3), np.zeros(3),
                self.camera_matrix, self.dist_coeffs
            )
            
            return image_points[0][0]
            
        except Exception as e:
            self.get_logger().error(f"Projection failed: {e}")
            return None
        


    def publish_robot_center(self, center, num_markers):
        """ë¡œë´‡ ì¤‘ì‹¬ ìœ„ì¹˜ ë°œí–‰"""
        msg = PoseStamped()
        msg.header = Header()
        msg.header.stamp = self.get_clock().now().to_msg()
        msg.header.frame_id = 'world'
        
        msg.pose.position.x = float(center[0])
        msg.pose.position.y = float(center[1])
        msg.pose.position.z = float(center[2])
        msg.pose.orientation.w = 1.0
        
        self.robot_center_pub.publish(msg)
        
        self.get_logger().info(
            f"Published robot center: ({center[0]:.3f}, {center[1]:.3f}, {center[2]:.3f}) "
            f"from {num_markers} markers"
        )
        
    def broadcast_robot_transform(self, H_world2robot, timestamp):
        """ë¡œë´‡ ì¢Œí‘œê³„ tf ë¸Œë¡œë“œìºìŠ¤íŠ¸"""
        # ì›”ë“œ â†’ ë¡œë´‡ ë³€í™˜ì„ ë¡œë´‡ â†’ ì›”ë“œë¡œ ë³€í™˜
        H_robot2world = np.linalg.inv(H_world2robot)
        
        t = TransformStamped()
        t.header.stamp = timestamp
        t.header.frame_id = 'world'
        t.child_frame_id = 'robot_center'
        
        # ìœ„ì¹˜
        t.transform.translation.x = float(H_world2robot[0, 3])
        t.transform.translation.y = float(H_world2robot[1, 3])
        t.transform.translation.z = float(H_world2robot[2, 3])
        
        # íšŒì „
        R_mat = H_robot2world[0:3, 0:3]
        rot = R.from_matrix(R_mat)
        quat = rot.as_quat()  # [x, y, z, w]
        t.transform.rotation.x = quat[0]
        t.transform.rotation.y = quat[1]
        t.transform.rotation.z = quat[2]
        t.transform.rotation.w = quat[3]
        
        self.tf_broadcaster.sendTransform(t)

    def broadcast_world_transform(self, H_world2cam, timestamp):
        """ì›”ë“œ ì¢Œí‘œê³„ tf ë¸Œë¡œë“œìºìŠ¤íŠ¸"""
        # ì¹´ë©”ë¼ â†’ ì›”ë“œ ë³€í™˜
        H_cam2world = np.linalg.inv(H_world2cam)
        
        t = TransformStamped()
        t.header.stamp = timestamp
        t.header.frame_id = 'camera_link'
        t.child_frame_id = 'world'
        
        # ìœ„ì¹˜
        t.transform.translation.x = float(H_cam2world[0, 3])
        t.transform.translation.y = float(H_cam2world[1, 3])
        t.transform.translation.z = float(H_cam2world[2, 3])
        
        # íšŒì „
        R_mat = H_cam2world[0:3, 0:3]
        rot = R.from_matrix(R_mat)
        quat = rot.as_quat()  # [x, y, z, w]
        t.transform.rotation.x = quat[0]
        t.transform.rotation.y = quat[1]
        t.transform.rotation.z = quat[2]
        t.transform.rotation.w = quat[3]
        
        self.tf_broadcaster.sendTransform(t)

    def calculate_robot_rotation_simple(self, robot_markers_cam):
        """ê°„ë‹¨í•œ ì•„ì›ƒë¼ì´ì–´ ì œê±° + íšŒì „ í‰ê· """
        
        if len(robot_markers_cam) <= 1:
            return list(robot_markers_cam.values())[0]['rotation']
        
        from scipy.spatial.transform import Rotation as R_scipy
        
        # Quaternion ë³€í™˜
        rotations_data = []
        for marker_id, data in robot_markers_cam.items():
            rot_matrix = data['rotation']
            r = R_scipy.from_matrix(rot_matrix)
            quat = r.as_quat()
            rotations_data.append((marker_id, quat, rot_matrix))
        
        # ê°„ë‹¨í•œ ì•„ì›ƒë¼ì´ì–´ ì œê±°: ì²« ë²ˆì§¸ì™€ ë„ˆë¬´ ë‹¤ë¥¸ ê²ƒ ì œê±°
        reference_quat = rotations_data[0][1]
        valid_rotations = []
        
        for marker_id, quat, rot_matrix in rotations_data:
            # ê°ë„ ì°¨ì´ ê³„ì‚°
            dot_product = np.abs(np.dot(reference_quat, quat))
            angle_diff = 2 * np.arccos(np.clip(dot_product, 0, 1))
            
            if angle_diff <= 0.5:  # ì•½ 30ë„ ì´ë‚´
                valid_rotations.append(quat)
            else:
                self.get_logger().warn(f"âš ï¸ Marker {marker_id} removed: angle diff {np.degrees(angle_diff):.1f}Â°")
        
        # í‰ê·  ê³„ì‚°
        if len(valid_rotations) > 0:
            mean_quat = np.mean(valid_rotations, axis=0)
            mean_quat = mean_quat / np.linalg.norm(mean_quat)
            return R_scipy.from_quat(mean_quat).as_matrix()
        else:
            return rotations_data[0][2]  # ì²« ë²ˆì§¸ ë§ˆì»¤ ì‚¬ìš©
    
    def visualize_results(self, image, detected_markers):
        """ê²€ì¶œ ê²°ê³¼ì™€ ì¢Œí‘œê³„ ì¶• ì‹œê°í™”"""
        
        # ê²€ì¶œëœ ë§ˆì»¤ë“¤ í‘œì‹œ
        for marker_id, data in detected_markers.items():
            corners = data['corners'][0]
            
            
            
            # ë§ˆì»¤ ID í‘œì‹œ - ìƒ‰ìƒ êµ¬ë¶„
            center = np.mean(corners, axis=0).astype(int)
            
            if marker_id == 10:
                # 10ë²ˆ ë§ˆì»¤ - ì›”ë“œ ì›ì  (íŒŒë€ìƒ‰)
                color = (255, 0, 0)
                label = f"World{marker_id}"
            # elif marker_id in self.robot_markers_local:
            #     # ë¡œë´‡ ë§ˆì»¤ë“¤ (ë¹¨ê°„ìƒ‰)
            #     color = (0, 0, 255)
            #     label = f"R{marker_id}"
                # ë§ˆì»¤ ê²½ê³„ ê·¸ë¦¬ê¸°
                cv2.polylines(image, [corners.astype(int)], True, (0, 255, 0), 2)
                cv2.putText(image, label, tuple(center), 
                        cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)
            
                # # ğŸ†• ë§ˆì»¤ë³„ ì¢Œí‘œì¶• ê·¸ë¦¬ê¸°
                self.draw_coordinate_axes(image, data['cam_rvec'], data['cam_tvec'], 0.05)

        # ğŸ¤– ë¡œë´‡ ì¤‘ì‹¬ê³¼ ë¡œë´‡ ì¢Œí‘œì¶• í‘œì‹œ
        if (self.world_established and 
            self.latest_robot_center is not None and
            self.center_timestamp is not None and 
            time.time() - self.center_timestamp < 1.0):
            
            # ë¡œë´‡ ì¤‘ì‹¬ì„ ì´ë¯¸ì§€ë¡œ íˆ¬ì˜
            center_image = self.project_world_to_image_simple(self.latest_robot_center)
            if center_image is not None:
                center_pt = tuple(center_image.astype(int))
                
                # ë¡œë´‡ ì¤‘ì‹¬ í‘œì‹œ
                cv2.circle(image, center_pt, 8, (0, 255, 255), -1)
                cv2.putText(image, "Robot", (center_pt[0]+15, center_pt[1]), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
                
                # ğŸ†• ë¡œë´‡ ì¢Œí‘œì¶• ê·¸ë¦¬ê¸°
                self.draw_robot_coordinate_axes(image)

        # ğŸ“Š ê°„ë‹¨í•œ ìƒíƒœ ì •ë³´ë§Œ
        world_status = "World: OK" if self.world_established else "World: No Marker 10"
        world_color = (0, 255, 0) if self.world_established else (0, 0, 255)
        cv2.putText(image, world_status, (10, 30), 
                cv2.FONT_HERSHEY_SIMPLEX, 0.7, world_color, 2)
        
        # ë¡œë´‡ ì¢Œí‘œ (ê°„ë‹¨íˆ)
        if self.latest_robot_center is not None:
            coord_text = f"Robot: ({self.latest_robot_center[0]:.2f}, {self.latest_robot_center[1]:.2f}, {self.latest_robot_center[2]:.2f})"
            cv2.putText(image, coord_text, (10, 60), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)

        # ì´ë¯¸ì§€ í‘œì‹œ
        cv2.imshow('ArUco Detection with Coordinate Axes', image)
        cv2.waitKey(1)

    def calculate_robot_center_robust(self, robot_markers_cam):
        """ë§ˆì»¤ ì¼ë¶€ê°€ ë³´ì´ì§€ ì•Šì•„ë„ ì•ˆì •ì ì¸ ë¡œë´‡ ì¤‘ì‹¬ ê³„ì‚°"""
        
        if len(robot_markers_cam) == 0:
            return None, None
        
        # ê²€ì¶œëœ ë§ˆì»¤ë“¤ì˜ ì¹´ë©”ë¼ ì¢Œí‘œ
        detected_markers = {}
        for marker_id, data in robot_markers_cam.items():
            detected_markers[marker_id] = data['position']  # ì¹´ë©”ë¼ ì¢Œí‘œê³„ì—ì„œì˜ ìœ„ì¹˜
        
        # ë°©ë²• 1: ë‹¨ìˆœ í‰ê·  (2ê°œ ì´ìƒ)
        if len(detected_markers) >= 2:
            robot_center_cam = self.estimate_center_from_partial_markers(detected_markers)
            robot_rotation_cam = self.calculate_robot_rotation_simple(robot_markers_cam)
            return robot_center_cam, robot_rotation_cam
        
        # ë°©ë²• 2: 1ê°œ ë§ˆì»¤ë§Œ ìˆì„ ë•Œ - í•´ë‹¹ ë§ˆì»¤ì—ì„œ ì¤‘ì‹¬ê¹Œì§€ì˜ ë²¡í„° ê³„ì‚°
        elif len(detected_markers) == 1:
            marker_id = list(detected_markers.keys())[0]
            marker_pos_cam = detected_markers[marker_id]
            marker_rotation_cam = robot_markers_cam[marker_id]['rotation']
            
            # ë§ˆì»¤ ì¢Œí‘œê³„ì—ì„œ ë¡œë´‡ ì¤‘ì‹¬ê¹Œì§€ì˜ ë²¡í„°
            local_offset = -self.robot_markers_local[marker_id]  # ë§ˆì»¤â†’ì¤‘ì‹¬ ë²¡í„°
            
            # ë§ˆì»¤ì˜ íšŒì „ì„ ì ìš©í•˜ì—¬ ì¹´ë©”ë¼ ì¢Œí‘œê³„ë¡œ ë³€í™˜
            center_offset_cam = marker_rotation_cam @ local_offset
            robot_center_cam = marker_pos_cam + center_offset_cam
            
            return robot_center_cam, marker_rotation_cam
        
        return None, None

    def estimate_center_from_partial_markers(self, detected_markers):
        """ë¶€ë¶„ì ìœ¼ë¡œ ê²€ì¶œëœ ë§ˆì»¤ë“¤ë¡œë¶€í„° ë¡œë´‡ ì¤‘ì‹¬ ì¶”ì •"""
        
        marker_ids = list(detected_markers.keys())
        
        # ì¼€ì´ìŠ¤ë³„ ìµœì í™”ëœ ì¤‘ì‹¬ ê³„ì‚°
        if len(marker_ids) == 4:
            # ëª¨ë“  ë§ˆì»¤ ê²€ì¶œ - ë‹¨ìˆœ í‰ê· 
            return np.mean(list(detected_markers.values()), axis=0)
        
        elif len(marker_ids) == 3:
            # 3ê°œ ë§ˆì»¤ - ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ë” ì •í™•í•œ ì¶”ì •
            return self.estimate_center_from_three_markers(detected_markers)
        
        elif len(marker_ids) == 2:
            # 2ê°œ ë§ˆì»¤ - ê¸°í•˜í•™ì  ê´€ê³„ ì´ìš©
            return self.estimate_center_from_two_markers(detected_markers)
        
        else:
            # 1ê°œ ë§ˆì»¤ëŠ” ìœ„ì—ì„œ ì²˜ë¦¬ë¨
            return list(detected_markers.values())[0]

    def estimate_center_from_three_markers(self, detected_markers):
        """3ê°œ ë§ˆì»¤ë¡œë¶€í„° ì¤‘ì‹¬ ì¶”ì •"""
        marker_ids = list(detected_markers.keys())
        
        # ì—†ëŠ” ë§ˆì»¤ ì°¾ê¸°
        missing_marker = None
        for i in range(4):
            if i not in marker_ids:
                missing_marker = i
                break
        
        # ëŒ€ê°ì„  ê´€ê³„ ì´ìš©í•´ì„œ ì—†ëŠ” ë§ˆì»¤ ìœ„ì¹˜ ì¶”ì •
        if missing_marker is not None:
            estimated_missing = self.estimate_missing_marker_position(detected_markers, missing_marker)
            if estimated_missing is not None:
                # 4ê°œ ë§ˆì»¤ë¡œ ì¤‘ì‹¬ ê³„ì‚°
                all_positions = list(detected_markers.values()) + [estimated_missing]
                return np.mean(all_positions, axis=0)
        
        # ì¶”ì • ì‹¤íŒ¨ì‹œ ë‹¨ìˆœ í‰ê· 
        return np.mean(list(detected_markers.values()), axis=0)

    def estimate_center_from_two_markers(self, detected_markers):
        """2ê°œ ë§ˆì»¤ë¡œë¶€í„° ì¤‘ì‹¬ ì¶”ì •"""
        marker_ids = list(detected_markers.keys())
        
        # ëŒ€ê°ì„  ê´€ê³„ì¸ì§€ í™•ì¸
        if self.are_diagonal_markers(marker_ids):
            # ëŒ€ê°ì„  ë§ˆì»¤ë“¤ì˜ ì¤‘ì ì´ ë¡œë´‡ ì¤‘ì‹¬
            return np.mean(list(detected_markers.values()), axis=0)
        
        else:
            # ì¸ì ‘í•œ ë§ˆì»¤ë“¤ - ê¸°í•˜í•™ì  ê´€ê³„ ì´ìš©
            return self.estimate_center_from_adjacent_markers(detected_markers, marker_ids)

    def are_diagonal_markers(self, marker_ids):
        """ë‘ ë§ˆì»¤ê°€ ëŒ€ê°ì„  ê´€ê³„ì¸ì§€ í™•ì¸"""
        diagonal_pairs = [(0, 2), (1, 3)]  # ëŒ€ê°ì„  ìŒë“¤
        marker_set = set(marker_ids)
        
        for pair in diagonal_pairs:
            if set(pair) == marker_set:
                return True
        return False

    def estimate_center_from_adjacent_markers(self, detected_markers, marker_ids):
        """ì¸ì ‘í•œ 2ê°œ ë§ˆì»¤ë¡œë¶€í„° ì¤‘ì‹¬ ì¶”ì •"""
        
        # ì•Œë ¤ì§„ ë¡œì»¬ ì¢Œí‘œê³„ì—ì„œì˜ ê´€ê³„ ì´ìš©
        positions = list(detected_markers.values())
        pos1, pos2 = positions[0], positions[1]
        
        id1, id2 = marker_ids[0], marker_ids[1]
        
        # ë¡œì»¬ ì¢Œí‘œê³„ì—ì„œì˜ ê´€ê³„
        local1 = self.robot_markers_local[id1]
        local2 = self.robot_markers_local[id2]
        local_center = np.array([0, 0, 0])  # ë¡œë´‡ ì¤‘ì‹¬
        
        # 2D í‰ë©´ì—ì„œì˜ ê¸°í•˜í•™ì  ê´€ê³„ ì´ìš© (Z=0 ê°€ì •)
        # local1 + t1 * direction1 = center
        # local2 + t2 * direction2 = center
        # ì´ë¥¼ ì¹´ë©”ë¼ ì¢Œí‘œê³„ë¡œ ë³€í™˜
        
        # ë‹¨ìˆœí•œ ë°©ë²•: ë‘ ë§ˆì»¤ì˜ ì¤‘ì ì—ì„œ ê¸°í•˜í•™ì  ì˜¤í”„ì…‹ ì ìš©
        midpoint = (pos1 + pos2) / 2
        
        # ë¡œì»¬ ì¢Œí‘œê³„ì—ì„œì˜ ì¤‘ì ê³¼ ì‹¤ì œ ì¤‘ì‹¬ì˜ ì°¨ì´
        local_midpoint = (local1 + local2) / 2
        local_offset = local_center - local_midpoint
        
        # ë§ˆì»¤ë“¤ì˜ ë°©í–¥ì„ ì¶”ì •í•´ì„œ ì˜¤í”„ì…‹ ì ìš© (ê°„ë‹¨í•œ ê·¼ì‚¬)
        direction = pos2 - pos1
        direction_norm = direction / np.linalg.norm(direction)
        
        # ë¡œì»¬ ì˜¤í”„ì…‹ì„ ì¹´ë©”ë¼ ì¢Œí‘œê³„ë¡œ ê·¼ì‚¬ ë³€í™˜
        # (ì •í™•í•œ íšŒì „ ì—†ì´ ëŒ€ëµì ì¸ ìŠ¤ì¼€ì¼ë§Œ ì ìš©)
        camera_scale = np.linalg.norm(direction) / np.linalg.norm(local2 - local1)
        estimated_offset = local_offset * camera_scale
        
        return midpoint + estimated_offset

    def estimate_missing_marker_position(self, detected_markers, missing_id):
        """ëŒ€ê°ì„  ê´€ê³„ë¥¼ ì´ìš©í•´ì„œ ì—†ëŠ” ë§ˆì»¤ ìœ„ì¹˜ ì¶”ì •"""
        
        # ëŒ€ê°ì„  ê´€ê³„: 0â†”2, 1â†”3
        diagonal_map = {0: 2, 1: 3, 2: 0, 3: 1}
        diagonal_partner = diagonal_map.get(missing_id)
        
        if diagonal_partner in detected_markers:
            # ëŒ€ê°ì„  íŒŒíŠ¸ë„ˆê°€ ìˆìœ¼ë©´, ë‚˜ë¨¸ì§€ ë‘ ë§ˆì»¤ë¡œë¶€í„° ì¶”ì •
            partner_pos = detected_markers[diagonal_partner]
            other_markers = {k: v for k, v in detected_markers.items() if k != diagonal_partner}
            
            if len(other_markers) >= 2:
                # í‰í–‰ì‚¬ë³€í˜• ê´€ê³„ ì´ìš©
                other_positions = list(other_markers.values())
                # missing = partner + (other1 - other2) or similar geometric relation
                
                # ë¡œì»¬ ì¢Œí‘œê³„ì—ì„œì˜ ê´€ê³„ë¥¼ ì´ìš©í•œ ì¶”ì •
                return self.estimate_by_parallelogram_rule(partner_pos, other_positions, missing_id, diagonal_partner)
        
        return None

    def estimate_by_parallelogram_rule(self, diagonal_pos, other_positions, missing_id, diagonal_id):
        """í‰í–‰ì‚¬ë³€í˜• ë²•ì¹™ìœ¼ë¡œ missing marker ìœ„ì¹˜ ì¶”ì •"""
        
        if len(other_positions) < 2:
            return None
        
        # ë¡œì»¬ ì¢Œí‘œê³„ì—ì„œì˜ ë²¡í„° ê´€ê³„
        local_missing = self.robot_markers_local[missing_id]
        local_diagonal = self.robot_markers_local[diagonal_id]
        local_center = np.array([0, 0, 0])
        
        # ëŒ€ê°ì„  ë²¡í„°
        local_diag_vector = local_diagonal - local_missing
        
        # ì¹´ë©”ë¼ ì¢Œí‘œê³„ì—ì„œ ëŒ€ê°ì„  ë²¡í„°ì˜ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ì¶”ì •
        # (ê°„ë‹¨í•œ ê·¼ì‚¬: ìŠ¤ì¼€ì¼ê³¼ ë°©í–¥ë§Œ ê³ ë ¤)
        other_center = np.mean(other_positions, axis=0)
        camera_center_approx = (diagonal_pos + other_center) / 2
        
        # ëŒ€ê°ì„  ê´€ê³„ë¡œ missing marker ì¶”ì •
        estimated_missing = 2 * camera_center_approx - diagonal_pos
        
        return estimated_missing
        
    def draw_robot_coordinate_axes(self, image):
        """ë¡œë´‡ ì¢Œí‘œì¶•ì„ draw_coordinate_axes í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œ ê·¸ë¦¬ê¸°"""
        
        if self.H_cam2robot is None:
            return
        
        try:
            # H_cam2robotì—ì„œ rvec, tvec ì¶”ì¶œ
            robot_rotation = self.H_cam2robot[0:3, 0:3]  # íšŒì „ í–‰ë ¬
            robot_translation = self.H_cam2robot[0:3, 3]  # ì´ë™ ë²¡í„°
            
            # íšŒì „ í–‰ë ¬ â†’ rodrigues ë²¡í„° ë³€í™˜
            robot_rvec, _ = cv2.Rodrigues(robot_rotation)
            robot_tvec = robot_translation
            
            # âœ… ê¸°ì¡´ draw_coordinate_axes í•¨ìˆ˜ ì‚¬ìš©
            self.draw_coordinate_axes(image, robot_rvec, robot_tvec, length=0.08)
            
            self.get_logger().info("âœ… Robot coordinate axes drawn successfully")
            
        except Exception as e:
            self.get_logger().warn(f"âŒ Failed to draw robot coordinate axes: {e}")
            
    def draw_coordinate_axes(self, image, rvec, tvec, length=0.03):
        """ë§ˆì»¤ì˜ ì¢Œí‘œì¶•ì„ ì´ë¯¸ì§€ì— ê·¸ë¦¬ê¸°"""
        try:
            # ì¶• í¬ì¸íŠ¸ë“¤ ì •ì˜ (ë§ˆì»¤ ì¤‘ì‹¬ì—ì„œ ê° ì¶• ë°©í–¥ìœ¼ë¡œ)
            axis_points = np.float32([[0,0,0], [length,0,0], [0,length,0], [0,0,length]]).reshape(-1,3)
            
            # 3D ì ë“¤ì„ ì´ë¯¸ì§€ë¡œ íˆ¬ì˜
            imgpts, _ = cv2.projectPoints(axis_points, rvec, tvec, 
                                        self.camera_matrix, self.dist_coeffs)
            
            imgpts = np.int32(imgpts).reshape(-1,2)
            
            # ì›ì 
            origin = tuple(imgpts[0])
            
            # Xì¶• (ë¹¨ê°„ìƒ‰)
            cv2.arrowedLine(image, origin, tuple(imgpts[1]), (0,0,255), 2)
            # Yì¶• (ì´ˆë¡ìƒ‰)  
            cv2.arrowedLine(image, origin, tuple(imgpts[2]), (0,255,0), 2)
            # Zì¶• (íŒŒë€ìƒ‰)
            cv2.arrowedLine(image, origin, tuple(imgpts[3]), (255,0,0), 2)
            
        except Exception as e:
            pass  # ì¡°ìš©íˆ ë¬´ì‹œ

def main(args=None):
    rclpy.init(args=args)
    detector = D435ArucoDetector()
    
    try:
        rclpy.spin(detector)
    except KeyboardInterrupt:
        pass
    finally:
        cv2.destroyAllWindows()
        detector.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()